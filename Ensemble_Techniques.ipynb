{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "What is Ensemble Learning in machine learning? Explain the key idea\n",
        "behind it.\n",
        "- Ensemble Learning in machine learning refers to a technique where multiple models (often called “weak learners”) are combined to create a stronger model that performs better than any individual model alone.\n",
        "- The core idea is that a group of diverse models working together can make more accurate and robust predictions than a single model\n",
        "- just like a committee’s decision is often better than one person’s judgment.\n"
      ],
      "metadata": {
        "id": "Ahe65RWCG_ft"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is the difference between Bagging and Boosting?\n",
        "- Bagging :-  \n",
        "    - Reduce variance\n",
        "    - Models are trained independently and in parallel on different random subsets of data\n",
        "    - Uses bootstrap sampling\n",
        "    - Final prediction is usually made by majority voting (classification)\n",
        "    - Treats all models equally\n",
        "- Boosting :-  \n",
        "    - Reduce bias\n",
        "    - Models are trained sequentially, each new model corrects errors of the previous one\n",
        "    - Uses weighted sampling, giving more weight to misclassified samples\n",
        "    - Final prediction is a weighted sum of all models\n",
        "    - Later models focus more on hard-to-classify examples"
      ],
      "metadata": {
        "id": "PwHUxBrQMQna"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What is bootstrap sampling and what role does it play in Bagging methods\n",
        "like Random Forest?\n",
        "- Bootstrap sampling is a statistical technique where we create multiple random samples from the original dataset\n",
        "- but with replacement.\n",
        "- Role in Bagging :-  \n",
        "     - Create diversity among models\n",
        "       - Each base model (e.g., decision tree) is trained on a different bootstrap sample of the data.\n",
        "       - This ensures models see slightly different subsets, reducing correlation between them.  \n",
        "     - Reduce variance and overfitting\n",
        "       - By averaging predictions from many such diverse models, the overall model becomes more stable and less prone to overfitting.\n",
        "     - Enable Out-of-Bag (OOB) error estimation\n",
        "       - Since some samples are left out of each bootstrap,these unused samples can be used to evaluate model performance without needing a separate validation set."
      ],
      "metadata": {
        "id": "XErO2ArtNaqb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "What are Out-of-Bag (OOB) samples and how is OOB score used to\n",
        "evaluate ensemble models?\n",
        "- When using bootstrap sampling in Bagging, each model (like a tree in a Random Forest) is trained on a bootstrap sample — a random sample with replacement from the training data.\n",
        "- Role of OOB Samples in Evaluation\n",
        "     - Each model (tree) can make predictions for the samples it didn’t see during\n",
        "     - training — i.e., its OOB samples.\n",
        "- for every data point:-\n",
        "     - Collect predictions from only those trees for which it was OOB.\n",
        "     - Compare these OOB predictions with the actual label.\n",
        "     - Average across all samples to get an OOB score (or OOB accuracy).\n"
      ],
      "metadata": {
        "id": "pOKxKlp2O1IF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Compare feature importance analysis in a single Decision Tree vs. a\n",
        "Random Forest.\n",
        "- Feature Importance in a Single Decision Tree:-\n",
        "    - A Decision Tree determines feature importance based on how much each feature contributes to reducing impurity (e.g., Gini impurity or entropy) across all its splits.\n",
        "- How It Works:-\n",
        "    - Every time it’s used to split a node, calculate how much that split reduces impurity.\n",
        "    - Sum up all those reductions across the tree.\n",
        "    - Normalize so that all importances add up to 1.0.\n",
        "- Feature Importance in a Random Forest:-\n",
        "   - A Random Forest is an ensemble of many decision trees built on different bootstrap samples and random feature subsets.\n",
        "- How It Works:-\n",
        "   - Compute feature importance for each tree\n",
        "   - Average the importance scores of each feature across all trees."
      ],
      "metadata": {
        "id": "nXeITlOQPzDh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ": Write a Python program to:\n",
        "● Load the Breast Cancer dataset using\n",
        "sklearn.datasets.load_breast_cancer()\n",
        "● Train a Random Forest Classifier\n",
        "● Print the top 5 most important features based on feature importance scores.\n",
        "-"
      ],
      "metadata": {
        "id": "Zqg4TWM3RZuP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Load dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "\n",
        "# Train Random Forest Classifier\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "rf.fit(X, y)\n",
        "\n",
        "# Get feature importances\n",
        "importances = rf.feature_importances_\n",
        "\n",
        "# Create a DataFrame for easy sorting and display\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "})\n",
        "\n",
        "# Sort by importance (descending)\n",
        "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
        "\n",
        "# Print top 5 most important features\n",
        "print(\"Top 5 Most Important Features:\\n\")\n",
        "print(feature_importance_df.head(5))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRyGO-GSRttx",
        "outputId": "ee79b0da-b566-42cd-c115-51a2938faff7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 5 Most Important Features:\n",
            "\n",
            "                 Feature  Importance\n",
            "23            worst area    0.139357\n",
            "27  worst concave points    0.132225\n",
            "7    mean concave points    0.107046\n",
            "20          worst radius    0.082848\n",
            "22       worst perimeter    0.080850\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to:\n",
        "● Train a Bagging Classifier using Decision Trees on the Iris dataset\n",
        "● Evaluate its accuracy and compare with a single Decision Tree\n"
      ],
      "metadata": {
        "id": "WRcG3Q7eR9se"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import BaggingClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# -------------------------------\n",
        "# 1️⃣ Train a Single Decision Tree\n",
        "# -------------------------------\n",
        "dt = DecisionTreeClassifier(random_state=42)\n",
        "dt.fit(X_train, y_train)\n",
        "y_pred_dt = dt.predict(X_test)\n",
        "dt_accuracy = accuracy_score(y_test, y_pred_dt)\n",
        "\n",
        "# -------------------------------\n",
        "# 2️⃣ Train a Bagging Classifier (with Decision Trees)\n",
        "# -------------------------------\n",
        "bagging_clf = BaggingClassifier(\n",
        "    estimator=DecisionTreeClassifier(),  # updated parameter name\n",
        "    n_estimators=50,\n",
        "    random_state=42\n",
        ")\n",
        "bagging_clf.fit(X_train, y_train)\n",
        "y_pred_bag = bagging_clf.predict(X_test)\n",
        "bag_accuracy = accuracy_score(y_test, y_pred_bag)\n",
        "\n",
        "# -------------------------------\n",
        "# 3️⃣ Compare Accuracies\n",
        "# -------------------------------\n",
        "print(\"Accuracy of Single Decision Tree: {:.2f}%\".format(dt_accuracy * 100))\n",
        "print(\"Accuracy of Bagging Classifier:   {:.2f}%\".format(bag_accuracy * 100))\n",
        "\n",
        "improvement = (bag_accuracy - dt_accuracy) * 100\n",
        "print(\"\\nAccuracy Improvement: {:.2f}%\".format(improvement))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTC2nhNnSGRW",
        "outputId": "0b0b314b-7c36-4fd6-b216-33a1cfd0c61a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of Single Decision Tree: 100.00%\n",
            "Accuracy of Bagging Classifier:   100.00%\n",
            "\n",
            "Accuracy Improvement: 0.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to:\n",
        "● Train a Random Forest Classifier\n",
        "● Tune hyperparameters max_depth and n_estimators using GridSearchCV\n",
        "● Print the best parameters and final accuracy"
      ],
      "metadata": {
        "id": "S203b3_jSyNP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# 1️⃣ Load the dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 2️⃣ Define the Random Forest model\n",
        "rf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# 3️⃣ Define the parameter grid to search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150],  # number of trees\n",
        "    'max_depth': [3, 5, 7, None]     # depth of trees\n",
        "}\n",
        "\n",
        "# 4️⃣ Set up GridSearchCV\n",
        "grid_search = GridSearchCV(\n",
        "    estimator=rf,\n",
        "    param_grid=param_grid,\n",
        "    cv=5,                # 5-fold cross-validation\n",
        "    scoring='accuracy',\n",
        "    n_jobs=-1            # use all CPU cores for speed\n",
        ")\n",
        "\n",
        "# 5️⃣ Fit GridSearchCV to training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# 6️⃣ Print best parameters and best score\n",
        "print(\"Best Parameters:\", grid_search.best_params_)\n",
        "print(\"Best Cross-Validation Accuracy: {:.2f}%\".format(grid_search.best_score_ * 100))\n",
        "\n",
        "# 7️⃣ Evaluate on the test set using best model\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred = best_model.predict(X_test)\n",
        "final_accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Final Test Accuracy: {:.2f}%\".format(final_accuracy * 100))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei8HspdTSW8W",
        "outputId": "597b514f-dbe0-410a-fc17-d261e5303654"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': 3, 'n_estimators': 150}\n",
            "Best Cross-Validation Accuracy: 94.29%\n",
            "Final Test Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write a Python program to:\n",
        "● Train a Bagging Regressor and a Random Forest Regressor on the California\n",
        "Housing dataset\n",
        "● Compare their Mean Squared Errors (MSE)"
      ],
      "metadata": {
        "id": "GD1R-ezhTbjf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import required libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import BaggingRegressor, RandomForestRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# 1️⃣ Load the California Housing dataset\n",
        "data = fetch_california_housing()\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Split into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# 2️⃣ Train a Bagging Regressor using Decision Trees\n",
        "bagging_reg = BaggingRegressor(\n",
        "    estimator=DecisionTreeRegressor(),\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "bagging_reg.fit(X_train, y_train)\n",
        "\n",
        "# 3️⃣ Train a Random Forest Regressor\n",
        "rf_reg = RandomForestRegressor(\n",
        "    n_estimators=100,\n",
        "    random_state=42,\n",
        "    n_jobs=-1\n",
        ")\n",
        "rf_reg.fit(X_train, y_train)\n",
        "\n",
        "# 4️⃣ Make predictions\n",
        "y_pred_bag = bagging_reg.predict(X_test)\n",
        "y_pred_rf = rf_reg.predict(X_test)\n",
        "\n",
        "# 5️⃣ Compute Mean Squared Errors\n",
        "mse_bag = mean_squared_error(y_test, y_pred_bag)\n",
        "mse_rf = mean_squared_error(y_test, y_pred_rf)\n",
        "\n",
        "# 6️⃣ Compare results\n",
        "print(\"Mean Squared Error (Bagging Regressor): {:.4f}\".format(mse_bag))\n",
        "print(\"Mean Squared Error (Random Forest Regressor): {:.4f}\".format(mse_rf))\n",
        "\n",
        "# Optional: highlight which model performed better\n",
        "if mse_bag < mse_rf:\n",
        "    print(\"\\n✅ Bagging Regressor performed slightly better.\")\n",
        "else:\n",
        "    print(\"\\n✅ Random Forest Regressor performed slightly better.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tNujeu0WTaI5",
        "outputId": "6abec2c3-509f-461f-aa43-0062b45949d2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error (Bagging Regressor): 0.2568\n",
            "Mean Squared Error (Random Forest Regressor): 0.2565\n",
            "\n",
            "✅ Random Forest Regressor performed slightly better.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are working as a data scientist at a financial institution to predict loan\n",
        "default. You have access to customer demographic and transaction history data.\n",
        "You decide to use ensemble techniques to increase model performance.\n",
        "Explain your step-by-step approach to:\n",
        "● Choose between Bagging or Boosting\n",
        "● Handle overfitting\n",
        "● Select base models\n",
        "● Evaluate performance using cross-validation\n",
        "● Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "- 1 Decide: Bagging vs Boosting\n",
        "- Use Bagging: -\n",
        "    - Your main problem is variance (models overfit to noisy samples).\n",
        "    - You have many features and complex interactions, and want robust out-of-the-box performance.\n",
        "    - You want relatively simple interpretability (feature importance) and fast training in parallel.\n",
        "   - Label noise / mislabeling is a concern (bagging is more robust).\n",
        "- Use Boosting:-\n",
        "    - You need to reduce bias and squeeze maximum predictive power from tabular features.\n",
        "    - You can tune hyperparameters and accept a bit more sensitivity to label noise.\n",
        "    - You want best possible predictive accuracy for a production scoring model (boosters often win in accuracy)\n",
        "- 2 Handle overfitting: -  \n",
        "    - Use class weights in model (preferred) so boosting/tree models emphasize minority class without resampling artifacts.\n",
        "   - Resampling (SMOTE, ADASYN) only for algorithms that need it (avoid for tree ensembles usually).\n",
        "   - Threshold tuning: optimize decision threshold for business metric (expected monetary loss), not just accuracy.\n",
        "  - Cost-sensitive evaluation: compute expected loss using actual loss amounts (recoveries, write-offs) and tune operating point.\n",
        "- 3 Select base models:-\n",
        "  - Gradient Boosting Machines: LightGBM / XGBoost / CatBoost.\n",
        "  - Random Forest (bagging) — good as a variance-reducing baseline.\n",
        "  - Regularized Logistic Regression (good for interpretable baseline & meta-learner).\n",
        "  - Simple decision tree (as a weak learner optionally).\n",
        "  - (Optional) Neural network with tabular architecture if you have massive data.\n",
        "- 4 Evaluate performance using cross-validation: -\n",
        "   - Cross-validation strategy -\n",
        "      - If data is cross-sectional with no time order: use stratified K-fold CV (K=5 or 10) to preserve class ratio.\n",
        "       - If data has time dependence (likely): use time-series / rolling window CV\n",
        "    - Metrics to report -\n",
        "       - AUC-ROC (good overall ranking metric).\n",
        "       - AUC-PR (preferred when positives are rare).\n",
        "       - Precision / Recall at chosen threshold; Recall (sensitivity) often prioritized for detecting defaults.\n",
        "       - F1 if a balance is needed.\n",
        "       - Brier score and calibration plots (probability estimates matter for credit decisions).\n",
        "       - KS statistic (common in credit scoring).\n",
        "       - Expected monetary loss / profit — compute using confusion matrix and actual dollar values (best single business metric).\n",
        "       - Top-decile lift (how concentrated defaults are in top risk-decile) — used in credit industry.\n",
        "        - Provide confidence intervals (via repeated CV or bootstrapping) for metrics.\n",
        "- 5 Justify how ensemble learning improves decision-making in this real-world\n",
        "context.\n",
        "- Improves Predictive Accuracy: -\n",
        "   - Individual models (like a single decision tree or logistic regression) may capture only part of the data pattern.\n",
        "   - Ensembles—such as Random Forest (Bagging) or Gradient Boosting (Boosting)combine many weak or moderately strong models to produce a stronger, more accurate predictor.\n",
        "   - This leads to higher AUC-ROC, better recall of defaulters, and fewer false approvals.\n",
        "- Reduces Model Variance and Overfitting -\n",
        "   - Bagging techniques (e.g., Random Forest) average results across multiple models trained on random samples of data.\n",
        "   - This reduces variance and stabilizes predictions, making the system less sensitive to noise or outliers in customer data.\n",
        "  - As a result, credit decisions become more consistent and robust.\n",
        "- Captures Complex Nonlinear Relationships -\n",
        "   - Boosting methods (like XGBoost or LightGBM) sequentially learn from mistakes of earlier models.\n",
        "  - They can identify subtle interactions between demographic and transactional features—for example, combinations of spending patterns and credit utilization that signal potential default.\n",
        "   - This leads to richer, data-driven insights beyond what a single model can detect\n",
        "- Handles Imbalanced Data Effectively -\n",
        "   - Loan default data is usually imbalanced (few defaults, many non-defaults).\n",
        "   - Boosting algorithms focus more on difficult or minority cases, improving detection of high-risk borrowers without overly rejecting safe ones.\n",
        "   - This enhances risk discrimination and portfolio quality.\n",
        "- Increases Business Confidence and Stability -\n",
        "   - By combining multiple models, ensembles average out individual biases and random errors.\n",
        "    - Decision-makers get more stable probability estimates, leading to better loan pricing, credit limit assignment, and capital allocation.\n",
        "    - This supports regulatory compliance (e.g., Basel norms) by providing reliable risk scores."
      ],
      "metadata": {
        "id": "3nNSRUeMUBI1"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hQHH6e6gTK59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}